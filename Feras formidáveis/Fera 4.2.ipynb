{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Feras Formidáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Stop right now, thank you very much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desta atividade é implementar uma estratégia de Parada Antecipada (Early Stopping) no\n",
    "processo de treino da rede neural feita em PyTorch. Utilizou-se dados simples para verificar o funcionamento, empregando X camadas com uma função de ativação linear. Boa parte do algoritmo foi adaptada do material de aula do Daniel Cassar [1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Parada Antecipada é um método para estimar um número de épocas adequado no treinamento do algoritmo, evitando que ele sofra subajuste ou sobre-ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvimento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas necessárias [2-5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a rede neural, baseada em uma Multi Layer Perceptron (MLP), em que aplica-se transformações lineares nos dados das camadas com base em uma função de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_dados_entrada, camadas_ocultas, funcao_ativacao, num_targets\n",
    "    ):\n",
    "        # o método super recupera todas as funções anteriores (no caso da classe Module)\n",
    "        super().__init__()\n",
    "\n",
    "        arquitetura = []\n",
    "\n",
    "        # primeira camada oculta\n",
    "        arquitetura.append(nn.Linear(num_dados_entrada, camadas_ocultas[0]))\n",
    "        arquitetura.append(funcao_ativacao)\n",
    "\n",
    "        # demais camadas ocultas\n",
    "        for i in range(1, len(camadas_ocultas)):\n",
    "            arquitetura.append(nn.Linear(camadas_ocultas[i-1], camadas_ocultas[i]))\n",
    "            arquitetura.append(funcao_ativacao)\n",
    "\n",
    "        # camada de saída\n",
    "        arquitetura.append(nn.Linear(camadas_ocultas[-1], num_targets))\n",
    "\n",
    "        self.camadas = nn.Sequential(*arquitetura)\n",
    "\n",
    "    # similar ao método __call__    \n",
    "    def forward(self, x):\n",
    "        x = self.camadas(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciando a MLP, contendo 4 dados de entrada, 2 camadas ocultas, função de ativação sigmoidal e 1 dado como output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 4\n",
    "CAMADAS_OCULTAS = [2, 3]\n",
    "FUNCAO_ATIVACAO = nn.Sigmoid()\n",
    "NUM_DADOS_DE_SAIDA = 1  \n",
    "\n",
    "minha_mlp = MLP(\n",
    "    NUM_DADOS_DE_ENTRADA, CAMADAS_OCULTAS, FUNCAO_ATIVACAO, NUM_DADOS_DE_SAIDA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a classe Camada [1], em que os neurônios são agrupados e passando a informação através de camadas ocultas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate (ln)\n",
    "TAXA_DE_APRENDIZADO = 0.001 \n",
    "\n",
    "# modo de atualizar os parâmetros, usou-se o estocástico gradient descent (SGD)\n",
    "otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)\n",
    "\n",
    "# função a ser minimizada, usou-se a do erro quadrático médio (MSE)\n",
    "fn_perda = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "  [1.0, 0.0, -1.0, 2.3],\n",
    "  [3.0, -1.0, 0.5, 9.1],\n",
    "  [0.7, 8.6, 3.0, 2.6],\n",
    "  [1.0, 1.0, -1.0, 4.4],\n",
    "]\n",
    "\n",
    "y_true = [1, 0, 0.2, 0.5]\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y_true = torch.tensor(y_true).view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, definindo nossa rede neural classificadora, que vai organizar nossos dados em camadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2231)\n",
      "1 tensor(0.2226)\n",
      "2 tensor(0.2220)\n",
      "3 tensor(0.2214)\n",
      "4 tensor(0.2209)\n",
      "5 tensor(0.2203)\n",
      "6 tensor(0.2198)\n",
      "7 tensor(0.2192)\n",
      "8 tensor(0.2187)\n",
      "9 tensor(0.2181)\n",
      "10 tensor(0.2176)\n",
      "11 tensor(0.2171)\n",
      "12 tensor(0.2165)\n",
      "13 tensor(0.2160)\n",
      "14 tensor(0.2155)\n",
      "15 tensor(0.2150)\n",
      "16 tensor(0.2145)\n",
      "17 tensor(0.2139)\n",
      "18 tensor(0.2134)\n",
      "19 tensor(0.2129)\n",
      "20 tensor(0.2124)\n",
      "21 tensor(0.2119)\n",
      "22 tensor(0.2115)\n",
      "23 tensor(0.2110)\n",
      "24 tensor(0.2105)\n",
      "25 tensor(0.2100)\n",
      "26 tensor(0.2095)\n",
      "27 tensor(0.2090)\n",
      "28 tensor(0.2086)\n",
      "29 tensor(0.2081)\n",
      "30 tensor(0.2076)\n",
      "31 tensor(0.2072)\n",
      "32 tensor(0.2067)\n",
      "33 tensor(0.2063)\n",
      "34 tensor(0.2058)\n",
      "35 tensor(0.2054)\n",
      "36 tensor(0.2049)\n",
      "37 tensor(0.2045)\n",
      "38 tensor(0.2041)\n",
      "39 tensor(0.2036)\n",
      "40 tensor(0.2032)\n",
      "41 tensor(0.2028)\n",
      "42 tensor(0.2023)\n",
      "43 tensor(0.2019)\n",
      "44 tensor(0.2015)\n",
      "45 tensor(0.2011)\n",
      "46 tensor(0.2007)\n",
      "47 tensor(0.2003)\n",
      "48 tensor(0.1999)\n",
      "49 tensor(0.1995)\n",
      "50 tensor(0.1991)\n",
      "51 tensor(0.1987)\n",
      "52 tensor(0.1983)\n",
      "53 tensor(0.1979)\n",
      "54 tensor(0.1975)\n",
      "55 tensor(0.1971)\n",
      "56 tensor(0.1967)\n",
      "57 tensor(0.1963)\n",
      "58 tensor(0.1960)\n",
      "59 tensor(0.1956)\n",
      "60 tensor(0.1952)\n",
      "61 tensor(0.1948)\n",
      "62 tensor(0.1945)\n",
      "63 tensor(0.1941)\n",
      "64 tensor(0.1938)\n",
      "65 tensor(0.1934)\n",
      "66 tensor(0.1930)\n",
      "67 tensor(0.1927)\n",
      "68 tensor(0.1923)\n",
      "69 tensor(0.1920)\n",
      "70 tensor(0.1916)\n",
      "71 tensor(0.1913)\n",
      "72 tensor(0.1910)\n",
      "73 tensor(0.1906)\n",
      "74 tensor(0.1903)\n",
      "75 tensor(0.1899)\n",
      "76 tensor(0.1896)\n",
      "77 tensor(0.1893)\n",
      "78 tensor(0.1890)\n",
      "79 tensor(0.1886)\n",
      "80 tensor(0.1883)\n",
      "81 tensor(0.1880)\n",
      "82 tensor(0.1877)\n",
      "83 tensor(0.1874)\n",
      "84 tensor(0.1871)\n",
      "85 tensor(0.1867)\n",
      "86 tensor(0.1864)\n",
      "87 tensor(0.1861)\n",
      "88 tensor(0.1858)\n",
      "89 tensor(0.1855)\n",
      "90 tensor(0.1852)\n",
      "91 tensor(0.1849)\n",
      "92 tensor(0.1846)\n",
      "93 tensor(0.1844)\n",
      "94 tensor(0.1841)\n",
      "95 tensor(0.1838)\n",
      "96 tensor(0.1835)\n",
      "97 tensor(0.1832)\n",
      "98 tensor(0.1829)\n",
      "99 tensor(0.1826)\n"
     ]
    }
   ],
   "source": [
    "# período de treinamento\n",
    "NUM_EPOCAS = 100\n",
    "\n",
    "# redundante, mas avisa que está na etapa de treinamento\n",
    "minha_mlp.train()\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = minha_mlp(x)\n",
    "\n",
    "    # zero grad\n",
    "    otimizador.zero_grad()\n",
    "\n",
    "    # loss\n",
    "    loss = fn_perda(y_true, y_pred)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # atualiza parâmetros - passo\n",
    "    otimizador.step()\n",
    "\n",
    "    # mostra resultado (opcional)\n",
    "    print(epoca, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_teste = [\n",
    "  [2.1, 4.8, -0.4, -1.0],\n",
    "  [2.0, -1.9, 3.1, 7.3],\n",
    "  [0.0, 1.9, 8.6, 6.4],\n",
    "]\n",
    "\n",
    "y_teste = [1.3, 0.3, 8.4]\n",
    "\n",
    "x_teste = torch.tensor(x_teste)\n",
    "y_teste = torch.tensor(y_teste).view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (camadas): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=2, out_features=3, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate, avisa que a mlp está na etapa de previsão e mostra informações\n",
    "minha_mlp.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o dataset próprio da biblioteca *Seaborn* relacionado a plantas [6], contendo características como comprimento e largura das pétalas e sépalas de uma flor, indicando a espécie da planta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dentro do with indica que não usará os gradientes locais (no_grad()), pois já otimizou e gasta recurso\n",
    "with torch.no_grad():\n",
    "    y_pred = minha_mlp(x_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando o dataset em dados de entrada e os valores reais, a fim de comparar com as previsões do modelo, além de definir uma rede neural classificadora com base na arquitetura da rede escolhida. De acordo com o dataset, há 4 atributos do problema, sendo os dados de entrada, e o algoritmo tem 1 dado de saída contendo a previsão da espécie (valor entre 0 e 1, sendo setosa mais próximo de 0 e virginica mais próximo de 1). Note que o valor real *y_true* foi atribuido na classe Valor, para permitir calcular a função de perda e os gradientes locais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2127],\n",
       "        [0.2488],\n",
       "        [0.2660]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que todos os valores estão bem próximos e em apenas uma categoria, indicando que o modelo só previu dados para o rótulo 0 (espécie setosa), mesmo com os dados de entrada contendo plantas das 2 espécies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi possível treinar uma rede neural em python puro para tarefas de classificação, embora ela não esteja bem otimizada, aumentando a perda ao longo do treinamento e prevendo apenas um rótulo para os dados do problema. Mesmo assim, foi interessante aprender sobre esse tipo de problema, aumentando meu conhecimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] CASSAR, Daniel. \"ATP-303 NN 4.2 - Notebook MLP.ipynb\". Material de Aula, 2025.\n",
    "\n",
    "[2] Biblioteca Math. https://docs.python.org/3/library/math.html\n",
    "\n",
    "[3] Biblioteca Random. https://docs.python.org/3/library/random.html\n",
    "\n",
    "[4] Biblioteca Pandas. https://pandas.pydata.org/docs/user_guide/index.html#user-guide\n",
    "\n",
    "[5] Biblioteca Seaborn. https://seaborn.pydata.org/\n",
    "\n",
    "[6] Dataset estudado. https://archive.ics.uci.edu/dataset/53/iris\n",
    "\n",
    "[7] Função de perda analisada. https://medium.com/ensina-ai/uma-explicação-visual-para-função-de-custo-binary-cross-entropy-ou-log-loss-eaee662c396c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
