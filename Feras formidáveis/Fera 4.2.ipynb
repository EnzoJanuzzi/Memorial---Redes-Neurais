{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Feras Formidáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Stop right now, thank you very much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desta atividade é implementar uma estratégia de Parada Antecipada (Early Stopping) no\n",
    "processo de treino da rede neural feita em PyTorch. Utilizou-se dados simples para verificar o funcionamento, empregando X camadas com uma função de ativação linear. Boa parte do algoritmo foi adaptada dos materiais de aula do Daniel Cassar [1,2].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Parada Antecipada é um método para estimar um número de épocas adequado no treinamento do algoritmo, evitando que ele sofra subajuste ou sobre-ajuste. Isso significa que, no final de cada época, deve-se calcular a precisão da classificação nos dados de validação e terminar o treinamento quando a precisão parar de melhorar. [3] Ou seja, em problemas de regressão, é necessário criar uma curva de aprendizado, verificando a progressão da função perda para os dados de treino e de validação e definindo qual a melhor época para parar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvimento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas necessárias [4-7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a rede neural, baseada em uma Multi Layer Perceptron (MLP), em que aplica-se transformações lineares nos dados das camadas com base em uma função de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_dados_entrada, camadas_ocultas, funcao_ativacao, num_targets\n",
    "    ):\n",
    "        # o método super recupera todas as funções anteriores (no caso da classe Module)\n",
    "        super().__init__()\n",
    "\n",
    "        arquitetura = []\n",
    "\n",
    "        # primeira camada oculta\n",
    "        arquitetura.append(nn.Linear(num_dados_entrada, camadas_ocultas[0]))\n",
    "        arquitetura.append(funcao_ativacao)\n",
    "\n",
    "        # demais camadas ocultas\n",
    "        for i in range(1, len(camadas_ocultas)):\n",
    "            arquitetura.append(nn.Linear(camadas_ocultas[i-1], camadas_ocultas[i]))\n",
    "            arquitetura.append(funcao_ativacao)\n",
    "\n",
    "        # camada de saída\n",
    "        arquitetura.append(nn.Linear(camadas_ocultas[-1], num_targets))\n",
    "\n",
    "        self.camadas = nn.Sequential(*arquitetura)\n",
    "\n",
    "    # similar ao método __call__    \n",
    "    def forward(self, x):\n",
    "        x = self.camadas(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciando a MLP, contendo 4 dados de entrada, 2 camadas ocultas, função de ativação sigmoidal e 1 dado como output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 4\n",
    "CAMADAS_OCULTAS = [2, 3]\n",
    "FUNCAO_ATIVACAO = nn.Sigmoid()\n",
    "NUM_DADOS_DE_SAIDA = 1  \n",
    "\n",
    "minha_mlp = MLP(\n",
    "    NUM_DADOS_DE_ENTRADA, CAMADAS_OCULTAS, FUNCAO_ATIVACAO, NUM_DADOS_DE_SAIDA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o otimizador do problema, o qual realiza as etapas de backpropagation, com uma taxa de aprendizado em 0.01 e o Erro Quadrático Médio (MSE) como função de perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate (ln)\n",
    "TAXA_DE_APRENDIZADO = 0.01 \n",
    "\n",
    "# modo de atualizar os parâmetros, usou-se o estocástico gradient descent (SGD)\n",
    "otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)\n",
    "\n",
    "# função a ser minimizada, usou-se a do erro quadrático médio (MSE)\n",
    "fn_perda = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando exemplos simples para testar o modelo, convertendo-os em tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino = [\n",
    "  [1.0, 0.0, -1.0, 2.3],\n",
    "  [3.0, -1.0, 0.5, 9.1],\n",
    "  [0.7, 8.6, 3.0, 2.6],\n",
    "  [1.0, 1.0, -1.0, 4.4],\n",
    "]\n",
    "y_treino = [1, 0, 0.2, 0.5]\n",
    "\n",
    "x_val = [\n",
    "  [6.0, 1.0, -6.0, 2.9],\n",
    "  [4.0, -2.5, 0.2, 7.8],\n",
    "  [-1.7, 5.6, 2.2, -3.4],\n",
    "  [8.0, 1.0, 1.0, 5.2],\n",
    "]\n",
    "y_val = [2, -1.3, 0.0, 3.5]\n",
    "\n",
    "x_treino = torch.tensor(x_treino)\n",
    "# Assim como o reshape pro numpy, converte em uma coluna (1), usando o número de linhas necessárias (-1)\n",
    "y_treino = torch.tensor(y_treino).view(-1,1)\n",
    "\n",
    "x_val = torch.tensor(x_val)\n",
    "y_val = torch.tensor(y_val).view(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando o modelo para os dados de treino e teste, definindo inicialmente um número de épocas como 1000, o que é exagerado para ilustrar o funcionamento da Parada Antecipada. O código foi adaptado de fontes online [8,9], definindo uma paciência de 1 e delta de 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parada Antecipada na época 292\n",
      "Época 0: Perda de treino 0.428693, perda de validação 4.711963\n",
      "Época 1: Perda de treino 0.408222, perda de validação 4.669833\n",
      "Época 2: Perda de treino 0.389189, perda de validação 4.629847\n",
      "Época 3: Perda de treino 0.371492, perda de validação 4.591882\n",
      "Época 4: Perda de treino 0.355040, perda de validação 4.555826\n",
      "Época 5: Perda de treino 0.339746, perda de validação 4.521574\n",
      "Época 6: Perda de treino 0.325528, perda de validação 4.489027\n",
      "Época 7: Perda de treino 0.312313, perda de validação 4.458090\n",
      "Época 8: Perda de treino 0.300030, perda de validação 4.428677\n",
      "Época 9: Perda de treino 0.288614, perda de validação 4.400704\n",
      "Época 10: Perda de treino 0.278005, perda de validação 4.374094\n",
      "Época 11: Perda de treino 0.268146, perda de validação 4.348774\n",
      "Época 12: Perda de treino 0.258984, perda de validação 4.324674\n",
      "Época 13: Perda de treino 0.250472, perda de validação 4.301730\n",
      "Época 14: Perda de treino 0.242563, perda de validação 4.279881\n",
      "Época 15: Perda de treino 0.235215, perda de validação 4.259068\n",
      "Época 16: Perda de treino 0.228389, perda de validação 4.239238\n",
      "Época 17: Perda de treino 0.222048, perda de validação 4.220340\n",
      "Época 18: Perda de treino 0.216157, perda de validação 4.202325\n",
      "Época 19: Perda de treino 0.210686, perda de validação 4.185147\n",
      "Época 20: Perda de treino 0.205604, perda de validação 4.168766\n",
      "Época 21: Perda de treino 0.200885, perda de validação 4.153138\n",
      "Época 22: Perda de treino 0.196502, perda de validação 4.138226\n",
      "Época 23: Perda de treino 0.192431, perda de validação 4.123994\n",
      "Época 24: Perda de treino 0.188651, perda de validação 4.110408\n",
      "Época 25: Perda de treino 0.185142, perda de validação 4.097436\n",
      "Época 26: Perda de treino 0.181883, perda de validação 4.085047\n",
      "Época 27: Perda de treino 0.178857, perda de validação 4.073213\n",
      "Época 28: Perda de treino 0.176048, perda de validação 4.061905\n",
      "Época 29: Perda de treino 0.173440, perda de validação 4.051100\n",
      "Época 30: Perda de treino 0.171018, perda de validação 4.040771\n",
      "Época 31: Perda de treino 0.168771, perda de validação 4.030896\n",
      "Época 32: Perda de treino 0.166684, perda de validação 4.021454\n",
      "Época 33: Perda de treino 0.164747, perda de validação 4.012424\n",
      "Época 34: Perda de treino 0.162949, perda de validação 4.003785\n",
      "Época 35: Perda de treino 0.161281, perda de validação 3.995519\n",
      "Época 36: Perda de treino 0.159732, perda de validação 3.987610\n",
      "Época 37: Perda de treino 0.158294, perda de validação 3.980040\n",
      "Época 38: Perda de treino 0.156960, perda de validação 3.972794\n",
      "Época 39: Perda de treino 0.155721, perda de validação 3.965856\n",
      "Época 40: Perda de treino 0.154572, perda de validação 3.959211\n",
      "Época 41: Perda de treino 0.153505, perda de validação 3.952848\n",
      "Época 42: Perda de treino 0.152515, perda de validação 3.946753\n",
      "Época 43: Perda de treino 0.151596, perda de validação 3.940914\n",
      "Época 44: Perda de treino 0.150744, perda de validação 3.935319\n",
      "Época 45: Perda de treino 0.149952, perda de validação 3.929957\n",
      "Época 46: Perda de treino 0.149218, perda de validação 3.924818\n",
      "Época 47: Perda de treino 0.148537, perda de validação 3.919891\n",
      "Época 48: Perda de treino 0.147904, perda de validação 3.915169\n",
      "Época 49: Perda de treino 0.147317, perda de validação 3.910640\n",
      "Época 50: Perda de treino 0.146773, perda de validação 3.906298\n",
      "Época 51: Perda de treino 0.146267, perda de validação 3.902133\n",
      "Época 52: Perda de treino 0.145798, perda de validação 3.898139\n",
      "Época 53: Perda de treino 0.145363, perda de validação 3.894307\n",
      "Época 54: Perda de treino 0.144959, perda de validação 3.890631\n",
      "Época 55: Perda de treino 0.144584, perda de validação 3.887103\n",
      "Época 56: Perda de treino 0.144236, perda de validação 3.883718\n",
      "Época 57: Perda de treino 0.143913, perda de validação 3.880471\n",
      "Época 58: Perda de treino 0.143613, perda de validação 3.877353\n",
      "Época 59: Perda de treino 0.143335, perda de validação 3.874361\n",
      "Época 60: Perda de treino 0.143076, perda de validação 3.871488\n",
      "Época 61: Perda de treino 0.142836, perda de validação 3.868731\n",
      "Época 62: Perda de treino 0.142614, perda de validação 3.866083\n",
      "Época 63: Perda de treino 0.142407, perda de validação 3.863541\n",
      "Época 64: Perda de treino 0.142215, perda de validação 3.861099\n",
      "Época 65: Perda de treino 0.142037, perda de validação 3.858755\n",
      "Época 66: Perda de treino 0.141872, perda de validação 3.856504\n",
      "Época 67: Perda de treino 0.141718, perda de validação 3.854341\n",
      "Época 68: Perda de treino 0.141575, perda de validação 3.852264\n",
      "Época 69: Perda de treino 0.141442, perda de validação 3.850269\n",
      "Época 70: Perda de treino 0.141319, perda de validação 3.848353\n",
      "Época 71: Perda de treino 0.141205, perda de validação 3.846511\n",
      "Época 72: Perda de treino 0.141098, perda de validação 3.844743\n",
      "Época 73: Perda de treino 0.141000, perda de validação 3.843043\n",
      "Época 74: Perda de treino 0.140908, perda de validação 3.841410\n",
      "Época 75: Perda de treino 0.140822, perda de validação 3.839841\n",
      "Época 76: Perda de treino 0.140743, perda de validação 3.838334\n",
      "Época 77: Perda de treino 0.140669, perda de validação 3.836884\n",
      "Época 78: Perda de treino 0.140600, perda de validação 3.835492\n",
      "Época 79: Perda de treino 0.140536, perda de validação 3.834154\n",
      "Época 80: Perda de treino 0.140477, perda de validação 3.832868\n",
      "Época 81: Perda de treino 0.140421, perda de validação 3.831632\n",
      "Época 82: Perda de treino 0.140370, perda de validação 3.830444\n",
      "Época 83: Perda de treino 0.140321, perda de validação 3.829303\n",
      "Época 84: Perda de treino 0.140277, perda de validação 3.828205\n",
      "Época 85: Perda de treino 0.140235, perda de validação 3.827151\n",
      "Época 86: Perda de treino 0.140196, perda de validação 3.826137\n",
      "Época 87: Perda de treino 0.140159, perda de validação 3.825163\n",
      "Época 88: Perda de treino 0.140125, perda de validação 3.824226\n",
      "Época 89: Perda de treino 0.140094, perda de validação 3.823325\n",
      "Época 90: Perda de treino 0.140064, perda de validação 3.822460\n",
      "Época 91: Perda de treino 0.140036, perda de validação 3.821628\n",
      "Época 92: Perda de treino 0.140011, perda de validação 3.820828\n",
      "Época 93: Perda de treino 0.139986, perda de validação 3.820059\n",
      "Época 94: Perda de treino 0.139964, perda de validação 3.819320\n",
      "Época 95: Perda de treino 0.139942, perda de validação 3.818610\n",
      "Época 96: Perda de treino 0.139922, perda de validação 3.817927\n",
      "Época 97: Perda de treino 0.139904, perda de validação 3.817271\n",
      "Época 98: Perda de treino 0.139886, perda de validação 3.816640\n",
      "Época 99: Perda de treino 0.139869, perda de validação 3.816033\n",
      "Época 100: Perda de treino 0.139854, perda de validação 3.815450\n",
      "Época 101: Perda de treino 0.139839, perda de validação 3.814890\n",
      "Época 102: Perda de treino 0.139825, perda de validação 3.814351\n",
      "Época 103: Perda de treino 0.139812, perda de validação 3.813833\n",
      "Época 104: Perda de treino 0.139800, perda de validação 3.813335\n",
      "Época 105: Perda de treino 0.139788, perda de validação 3.812857\n",
      "Época 106: Perda de treino 0.139777, perda de validação 3.812397\n",
      "Época 107: Perda de treino 0.139767, perda de validação 3.811955\n",
      "Época 108: Perda de treino 0.139757, perda de validação 3.811531\n",
      "Época 109: Perda de treino 0.139747, perda de validação 3.811123\n",
      "Época 110: Perda de treino 0.139738, perda de validação 3.810730\n",
      "Época 111: Perda de treino 0.139730, perda de validação 3.810354\n",
      "Época 112: Perda de treino 0.139721, perda de validação 3.809991\n",
      "Época 113: Perda de treino 0.139714, perda de validação 3.809643\n",
      "Época 114: Perda de treino 0.139706, perda de validação 3.809309\n",
      "Época 115: Perda de treino 0.139699, perda de validação 3.808988\n",
      "Época 116: Perda de treino 0.139692, perda de validação 3.808679\n",
      "Época 117: Perda de treino 0.139686, perda de validação 3.808383\n",
      "Época 118: Perda de treino 0.139679, perda de validação 3.808098\n",
      "Época 119: Perda de treino 0.139673, perda de validação 3.807824\n",
      "Época 120: Perda de treino 0.139667, perda de validação 3.807562\n",
      "Época 121: Perda de treino 0.139661, perda de validação 3.807309\n",
      "Época 122: Perda de treino 0.139656, perda de validação 3.807067\n",
      "Época 123: Perda de treino 0.139651, perda de validação 3.806835\n",
      "Época 124: Perda de treino 0.139645, perda de validação 3.806611\n",
      "Época 125: Perda de treino 0.139640, perda de validação 3.806396\n",
      "Época 126: Perda de treino 0.139635, perda de validação 3.806191\n",
      "Época 127: Perda de treino 0.139631, perda de validação 3.805994\n",
      "Época 128: Perda de treino 0.139626, perda de validação 3.805803\n",
      "Época 129: Perda de treino 0.139621, perda de validação 3.805621\n",
      "Época 130: Perda de treino 0.139617, perda de validação 3.805447\n",
      "Época 131: Perda de treino 0.139613, perda de validação 3.805279\n",
      "Época 132: Perda de treino 0.139608, perda de validação 3.805118\n",
      "Época 133: Perda de treino 0.139604, perda de validação 3.804964\n",
      "Época 134: Perda de treino 0.139600, perda de validação 3.804816\n",
      "Época 135: Perda de treino 0.139596, perda de validação 3.804675\n",
      "Época 136: Perda de treino 0.139592, perda de validação 3.804539\n",
      "Época 137: Perda de treino 0.139588, perda de validação 3.804408\n",
      "Época 138: Perda de treino 0.139584, perda de validação 3.804283\n",
      "Época 139: Perda de treino 0.139580, perda de validação 3.804163\n",
      "Época 140: Perda de treino 0.139576, perda de validação 3.804049\n",
      "Época 141: Perda de treino 0.139573, perda de validação 3.803939\n",
      "Época 142: Perda de treino 0.139569, perda de validação 3.803833\n",
      "Época 143: Perda de treino 0.139565, perda de validação 3.803733\n",
      "Época 144: Perda de treino 0.139562, perda de validação 3.803637\n",
      "Época 145: Perda de treino 0.139558, perda de validação 3.803544\n",
      "Época 146: Perda de treino 0.139554, perda de validação 3.803456\n",
      "Época 147: Perda de treino 0.139551, perda de validação 3.803372\n",
      "Época 148: Perda de treino 0.139547, perda de validação 3.803291\n",
      "Época 149: Perda de treino 0.139544, perda de validação 3.803214\n",
      "Época 150: Perda de treino 0.139540, perda de validação 3.803140\n",
      "Época 151: Perda de treino 0.139537, perda de validação 3.803070\n",
      "Época 152: Perda de treino 0.139533, perda de validação 3.803003\n",
      "Época 153: Perda de treino 0.139530, perda de validação 3.802938\n",
      "Época 154: Perda de treino 0.139527, perda de validação 3.802877\n",
      "Época 155: Perda de treino 0.139523, perda de validação 3.802818\n",
      "Época 156: Perda de treino 0.139520, perda de validação 3.802763\n",
      "Época 157: Perda de treino 0.139516, perda de validação 3.802710\n",
      "Época 158: Perda de treino 0.139513, perda de validação 3.802660\n",
      "Época 159: Perda de treino 0.139510, perda de validação 3.802611\n",
      "Época 160: Perda de treino 0.139506, perda de validação 3.802565\n",
      "Época 161: Perda de treino 0.139503, perda de validação 3.802522\n",
      "Época 162: Perda de treino 0.139500, perda de validação 3.802480\n",
      "Época 163: Perda de treino 0.139496, perda de validação 3.802441\n",
      "Época 164: Perda de treino 0.139493, perda de validação 3.802404\n",
      "Época 165: Perda de treino 0.139490, perda de validação 3.802369\n",
      "Época 166: Perda de treino 0.139486, perda de validação 3.802335\n",
      "Época 167: Perda de treino 0.139483, perda de validação 3.802304\n",
      "Época 168: Perda de treino 0.139480, perda de validação 3.802273\n",
      "Época 169: Perda de treino 0.139477, perda de validação 3.802245\n",
      "Época 170: Perda de treino 0.139473, perda de validação 3.802219\n",
      "Época 171: Perda de treino 0.139470, perda de validação 3.802193\n",
      "Época 172: Perda de treino 0.139467, perda de validação 3.802170\n",
      "Época 173: Perda de treino 0.139463, perda de validação 3.802147\n",
      "Época 174: Perda de treino 0.139460, perda de validação 3.802126\n",
      "Época 175: Perda de treino 0.139457, perda de validação 3.802107\n",
      "Época 176: Perda de treino 0.139454, perda de validação 3.802089\n",
      "Época 177: Perda de treino 0.139450, perda de validação 3.802072\n",
      "Época 178: Perda de treino 0.139447, perda de validação 3.802056\n",
      "Época 179: Perda de treino 0.139444, perda de validação 3.802041\n",
      "Época 180: Perda de treino 0.139441, perda de validação 3.802027\n",
      "Época 181: Perda de treino 0.139437, perda de validação 3.802015\n",
      "Época 182: Perda de treino 0.139434, perda de validação 3.802003\n",
      "Época 183: Perda de treino 0.139431, perda de validação 3.801993\n",
      "Época 184: Perda de treino 0.139427, perda de validação 3.801983\n",
      "Época 185: Perda de treino 0.139424, perda de validação 3.801974\n",
      "Época 186: Perda de treino 0.139421, perda de validação 3.801966\n",
      "Época 187: Perda de treino 0.139418, perda de validação 3.801960\n",
      "Época 188: Perda de treino 0.139414, perda de validação 3.801953\n",
      "Época 189: Perda de treino 0.139411, perda de validação 3.801948\n",
      "Época 190: Perda de treino 0.139408, perda de validação 3.801943\n",
      "Época 191: Perda de treino 0.139405, perda de validação 3.801939\n",
      "Época 192: Perda de treino 0.139401, perda de validação 3.801936\n",
      "Época 193: Perda de treino 0.139398, perda de validação 3.801933\n",
      "Época 194: Perda de treino 0.139395, perda de validação 3.801931\n",
      "Época 195: Perda de treino 0.139392, perda de validação 3.801929\n",
      "Época 196: Perda de treino 0.139388, perda de validação 3.801929\n",
      "Época 197: Perda de treino 0.139385, perda de validação 3.801929\n",
      "Época 198: Perda de treino 0.139382, perda de validação 3.801929\n",
      "Época 199: Perda de treino 0.139379, perda de validação 3.801930\n",
      "Época 200: Perda de treino 0.139375, perda de validação 3.801931\n",
      "Época 201: Perda de treino 0.139372, perda de validação 3.801933\n",
      "Época 202: Perda de treino 0.139369, perda de validação 3.801935\n",
      "Época 203: Perda de treino 0.139366, perda de validação 3.801938\n",
      "Época 204: Perda de treino 0.139362, perda de validação 3.801941\n",
      "Época 205: Perda de treino 0.139359, perda de validação 3.801945\n",
      "Época 206: Perda de treino 0.139356, perda de validação 3.801950\n",
      "Época 207: Perda de treino 0.139353, perda de validação 3.801954\n",
      "Época 208: Perda de treino 0.139349, perda de validação 3.801959\n",
      "Época 209: Perda de treino 0.139346, perda de validação 3.801963\n",
      "Época 210: Perda de treino 0.139343, perda de validação 3.801969\n",
      "Época 211: Perda de treino 0.139339, perda de validação 3.801975\n",
      "Época 212: Perda de treino 0.139336, perda de validação 3.801981\n",
      "Época 213: Perda de treino 0.139333, perda de validação 3.801987\n",
      "Época 214: Perda de treino 0.139330, perda de validação 3.801994\n",
      "Época 215: Perda de treino 0.139326, perda de validação 3.802001\n",
      "Época 216: Perda de treino 0.139323, perda de validação 3.802009\n",
      "Época 217: Perda de treino 0.139320, perda de validação 3.802016\n",
      "Época 218: Perda de treino 0.139316, perda de validação 3.802024\n",
      "Época 219: Perda de treino 0.139313, perda de validação 3.802032\n",
      "Época 220: Perda de treino 0.139310, perda de validação 3.802041\n",
      "Época 221: Perda de treino 0.139307, perda de validação 3.802049\n",
      "Época 222: Perda de treino 0.139303, perda de validação 3.802058\n",
      "Época 223: Perda de treino 0.139300, perda de validação 3.802067\n",
      "Época 224: Perda de treino 0.139297, perda de validação 3.802076\n",
      "Época 225: Perda de treino 0.139293, perda de validação 3.802086\n",
      "Época 226: Perda de treino 0.139290, perda de validação 3.802095\n",
      "Época 227: Perda de treino 0.139287, perda de validação 3.802105\n",
      "Época 228: Perda de treino 0.139284, perda de validação 3.802115\n",
      "Época 229: Perda de treino 0.139280, perda de validação 3.802125\n",
      "Época 230: Perda de treino 0.139277, perda de validação 3.802135\n",
      "Época 231: Perda de treino 0.139274, perda de validação 3.802146\n",
      "Época 232: Perda de treino 0.139270, perda de validação 3.802157\n",
      "Época 233: Perda de treino 0.139267, perda de validação 3.802167\n",
      "Época 234: Perda de treino 0.139264, perda de validação 3.802178\n",
      "Época 235: Perda de treino 0.139261, perda de validação 3.802189\n",
      "Época 236: Perda de treino 0.139257, perda de validação 3.802200\n",
      "Época 237: Perda de treino 0.139254, perda de validação 3.802211\n",
      "Época 238: Perda de treino 0.139251, perda de validação 3.802223\n",
      "Época 239: Perda de treino 0.139247, perda de validação 3.802234\n",
      "Época 240: Perda de treino 0.139244, perda de validação 3.802246\n",
      "Época 241: Perda de treino 0.139241, perda de validação 3.802258\n",
      "Época 242: Perda de treino 0.139237, perda de validação 3.802269\n",
      "Época 243: Perda de treino 0.139234, perda de validação 3.802281\n",
      "Época 244: Perda de treino 0.139231, perda de validação 3.802293\n",
      "Época 245: Perda de treino 0.139228, perda de validação 3.802305\n",
      "Época 246: Perda de treino 0.139224, perda de validação 3.802318\n",
      "Época 247: Perda de treino 0.139221, perda de validação 3.802330\n",
      "Época 248: Perda de treino 0.139218, perda de validação 3.802342\n",
      "Época 249: Perda de treino 0.139214, perda de validação 3.802355\n",
      "Época 250: Perda de treino 0.139211, perda de validação 3.802367\n",
      "Época 251: Perda de treino 0.139208, perda de validação 3.802380\n",
      "Época 252: Perda de treino 0.139204, perda de validação 3.802392\n",
      "Época 253: Perda de treino 0.139201, perda de validação 3.802405\n",
      "Época 254: Perda de treino 0.139198, perda de validação 3.802417\n",
      "Época 255: Perda de treino 0.139194, perda de validação 3.802430\n",
      "Época 256: Perda de treino 0.139191, perda de validação 3.802443\n",
      "Época 257: Perda de treino 0.139188, perda de validação 3.802456\n",
      "Época 258: Perda de treino 0.139184, perda de validação 3.802469\n",
      "Época 259: Perda de treino 0.139181, perda de validação 3.802482\n",
      "Época 260: Perda de treino 0.139178, perda de validação 3.802495\n",
      "Época 261: Perda de treino 0.139175, perda de validação 3.802508\n",
      "Época 262: Perda de treino 0.139171, perda de validação 3.802521\n",
      "Época 263: Perda de treino 0.139168, perda de validação 3.802535\n",
      "Época 264: Perda de treino 0.139165, perda de validação 3.802548\n",
      "Época 265: Perda de treino 0.139161, perda de validação 3.802561\n",
      "Época 266: Perda de treino 0.139158, perda de validação 3.802575\n",
      "Época 267: Perda de treino 0.139155, perda de validação 3.802588\n",
      "Época 268: Perda de treino 0.139151, perda de validação 3.802601\n",
      "Época 269: Perda de treino 0.139148, perda de validação 3.802615\n",
      "Época 270: Perda de treino 0.139145, perda de validação 3.802628\n",
      "Época 271: Perda de treino 0.139141, perda de validação 3.802641\n",
      "Época 272: Perda de treino 0.139138, perda de validação 3.802655\n",
      "Época 273: Perda de treino 0.139134, perda de validação 3.802669\n",
      "Época 274: Perda de treino 0.139131, perda de validação 3.802682\n",
      "Época 275: Perda de treino 0.139128, perda de validação 3.802696\n",
      "Época 276: Perda de treino 0.139124, perda de validação 3.802709\n",
      "Época 277: Perda de treino 0.139121, perda de validação 3.802723\n",
      "Época 278: Perda de treino 0.139118, perda de validação 3.802737\n",
      "Época 279: Perda de treino 0.139114, perda de validação 3.802751\n",
      "Época 280: Perda de treino 0.139111, perda de validação 3.802765\n",
      "Época 281: Perda de treino 0.139108, perda de validação 3.802778\n",
      "Época 282: Perda de treino 0.139104, perda de validação 3.802792\n",
      "Época 283: Perda de treino 0.139101, perda de validação 3.802806\n",
      "Época 284: Perda de treino 0.139098, perda de validação 3.802819\n",
      "Época 285: Perda de treino 0.139094, perda de validação 3.802833\n",
      "Época 286: Perda de treino 0.139091, perda de validação 3.802847\n",
      "Época 287: Perda de treino 0.139088, perda de validação 3.802861\n",
      "Época 288: Perda de treino 0.139084, perda de validação 3.802875\n",
      "Época 289: Perda de treino 0.139081, perda de validação 3.802888\n",
      "Época 290: Perda de treino 0.139077, perda de validação 3.802903\n",
      "Época 291: Perda de treino 0.139074, perda de validação 3.802917\n",
      "Época 292: Perda de treino 0.139071, perda de validação 3.802930\n"
     ]
    }
   ],
   "source": [
    "# Período de treinamento inicial\n",
    "NUM_EPOCAS = 1000\n",
    "epocas = [i for i in range(NUM_EPOCAS)]\n",
    "perdas_treino = []\n",
    "perdas_val = []\n",
    "\n",
    "# Parâmetros do early stopping\n",
    "PACIENCIA = 1\n",
    "DELTA = 0.001\n",
    "i = 0\n",
    "min_perda_val = float('inf')\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # Treinamento\n",
    "    minha_mlp.train()\n",
    "    yp_treino = minha_mlp(x_treino) # forward pass\n",
    "    otimizador.zero_grad() # zero grad\n",
    "    loss = fn_perda(y_treino, yp_treino) # função de perda\n",
    "    perdas_treino.append(loss)\n",
    "    loss.backward() # backpropagation\n",
    "    otimizador.step() # atualiza parâmetros - passo\n",
    "\n",
    "    # Validação\n",
    "    minha_mlp.eval()\n",
    "    with torch.no_grad():  # não calcula os gradientes locais\n",
    "        yp_val = minha_mlp(x_val)\n",
    "        val_loss = fn_perda(y_val, yp_val)\n",
    "        perdas_val.append(val_loss) \n",
    "\n",
    "    # Parada Antecipada\n",
    "    if val_loss < min_perda_val:\n",
    "        min_perda_val = val_loss\n",
    "        i = 0\n",
    "        \n",
    "    elif val_loss > (min_perda_val + DELTA):\n",
    "        i += 1\n",
    "        if i >= PACIENCIA:\n",
    "            print(f'Parada Antecipada na época {epoca}')\n",
    "            break\n",
    "\n",
    "for treino, val, epoca in zip(perdas_treino, perdas_val, epocas):\n",
    "    print(f'Época {epoca}: Perda de treino {treino:4f}, perda de validação {val:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que a estratégia funcionou, forçando a parada do treinamento quando a perda da validação começa a aumentar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_teste = [\n",
    "  [2.1, 4.8, -0.4, -1.0],\n",
    "  [2.0, -1.9, 3.1, 7.3],\n",
    "  [0.0, 1.9, 8.6, 6.4],\n",
    "]\n",
    "\n",
    "y_teste = [1.3, 0.3, 8.4]\n",
    "\n",
    "x_teste = torch.tensor(x_teste)\n",
    "y_teste = torch.tensor(y_teste).view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando a rede neural para os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4071],\n",
       "        [0.4309],\n",
       "        [0.4044]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate, avisa que a mlp está na etapa de previsão e mostra informações\n",
    "minha_mlp.eval()\n",
    "\n",
    "# dentro do with indica que não usará os gradientes locais (no_grad()), pois já otimizou e gasta recurso\n",
    "with torch.no_grad():\n",
    "    y_pred = minha_mlp(x_teste)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, as previsões são sub-ótimas, pois o foco está em implementar o método de Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi possível treinar uma rede neural em python puro para tarefas de classificação, embora ela não esteja bem otimizada, aumentando a perda ao longo do treinamento e prevendo apenas um rótulo para os dados do problema. Mesmo assim, foi interessante aprender sobre esse tipo de problema, aumentando meu conhecimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] CASSAR, Daniel. \"ATP-303 NN 5.2 - Notebook PyTorch.ipynb\". Material de Aula, 2025.\n",
    "\n",
    "[2] CASSAR, Daniel. \"ATP-303 NN 6.2 - Construindo e treinando uma rede neural com Lightning.ipynb\". Material de Aula, 2025.\n",
    "\n",
    "[3] Material sobre Early stopping. https://www.deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/\n",
    "\n",
    "[2] Biblioteca Math. https://docs.python.org/3/library/math.html\n",
    "\n",
    "[3] Biblioteca Random. https://docs.python.org/3/library/random.html\n",
    "\n",
    "[4] Biblioteca Pandas. https://pandas.pydata.org/docs/user_guide/index.html#user-guide\n",
    "\n",
    "[5] Biblioteca Seaborn. https://seaborn.pydata.org/\n",
    "\n",
    "[6] Dataset estudado. https://archive.ics.uci.edu/dataset/53/iris\n",
    "\n",
    "[7] Função de perda analisada. https://medium.com/ensina-ai/uma-explicação-visual-para-função-de-custo-binary-cross-entropy-ou-log-loss-eaee662c396c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
